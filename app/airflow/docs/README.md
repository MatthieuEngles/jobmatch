# üöÄ Airflow Orchestration - JobMatch

Orchestration du pipeline ETL offre-ingestion avec Apache Airflow.

## üìã Vue d'ensemble

Ce module orchestre le pipeline ETL d'ingestion des offres d'emploi France Travail :
1. **Fetch** : R√©cup√©ration des offres depuis l'API ‚Üí GCS
2. **Silver** : Transformation et nettoyage ‚Üí BigQuery Silver
3. **Gold** : Enrichissement avec embeddings ‚Üí BigQuery Gold

## üèóÔ∏è Architecture

```
airflow/
‚îú‚îÄ‚îÄ docker-compose.yml    # Configuration des services Airflow
‚îú‚îÄ‚îÄ Dockerfile            # Image Airflow personnalis√©e
‚îú‚îÄ‚îÄ requirements.txt      # D√©pendances Python
‚îú‚îÄ‚îÄ .env                  # Variables d'environnement
‚îú‚îÄ‚îÄ dags/                 # DAGs Airflow
‚îÇ   ‚îî‚îÄ‚îÄ offre_ingestion_dag.py
‚îú‚îÄ‚îÄ logs/                 # Logs d'ex√©cution
‚îú‚îÄ‚îÄ plugins/              # Plugins personnalis√©s
‚îî‚îÄ‚îÄ config/               # Configuration suppl√©mentaire
```

## üöÄ Quick Start

### 1. Configuration initiale

```bash
# Se placer dans le dossier airflow
cd /home/mohamede.madiouni/jobmatch/app/airflow

# Copier et adapter le fichier .env
cp .env .env.local
# √âditer .env.local pour pointer vers vos credentials GCP
```

### 2. Configurer les permissions (Linux uniquement)

```bash
# Obtenir votre UID
echo $(id -u)

# Mettre √† jour AIRFLOW_UID dans .env.local si n√©cessaire
echo "AIRFLOW_UID=$(id -u)" >> .env.local

# Cr√©er les dossiers avec les bonnes permissions
mkdir -p ./logs ./plugins
chmod -R 777 ./logs ./plugins
```

### 3. Build de l'image offre-ingestion (si pas d√©j√† fait)

```bash
cd /home/mohamede.madiouni/jobmatch
docker build -f app/offre-ingestion/Dockerfile -t offre-ingestion-pipeline:latest .
```

### 4. D√©marrage d'Airflow

```bash
cd /home/mohamede.madiouni/jobmatch/app/airflow

# Initialiser la base de donn√©es et cr√©er les services
docker compose up -d

# V√©rifier que tous les services sont lanc√©s
docker compose ps
```

### 5. Acc√©der √† l'interface Airflow

```
URL: http://localhost:8080
Username: airflow
Password: airflow
```

## üìä Utilisation

### Interface Web

1. Aller sur http://localhost:8080
2. Se connecter avec `airflow/airflow`
3. Activer le DAG `offre_ingestion_pipeline`
4. D√©clencher manuellement : bouton ‚ñ∂Ô∏è (Play)

### Configuration des variables Airflow

Le DAG n√©cessite la variable `GOOGLE_APPLICATION_CREDENTIALS` :

```bash
# Via CLI
docker compose exec airflow-webserver airflow variables set \
  GOOGLE_APPLICATION_CREDENTIALS \
  /opt/airflow/credentials/gcp-key.json

# Ou via l'interface Web :
Admin ‚Üí Variables ‚Üí + ‚Üí
Key: GOOGLE_APPLICATION_CREDENTIALS
Value: /opt/airflow/credentials/gcp-key.json
```

### Commandes utiles

```bash
# Voir les logs en temps r√©el
docker compose logs -f airflow-scheduler

# Lister les DAGs
docker compose exec airflow-webserver airflow dags list

# Tester un DAG sans l'ex√©cuter
docker compose exec airflow-webserver airflow dags test offre_ingestion_pipeline 2025-01-01

# Lancer manuellement une t√¢che sp√©cifique
docker compose exec airflow-webserver airflow tasks test offre_ingestion_pipeline fetch_offers_to_gcs 2025-01-01

# Arr√™ter Airflow
docker compose down

# Supprimer tout (y compris la base de donn√©es)
docker compose down -v
```

## ‚öôÔ∏è Configuration du DAG

### Scheduling

Par d√©faut : **Tous les jours √† 2h du matin**

Modifier dans [dags/offre_ingestion_dag.py](dags/offre_ingestion_dag.py):
```python
schedule_interval='0 2 * * *',  # Cron expression
```

### Param√®tres d'ex√©cution

- **Date d'ex√©cution** : `{{ ds }}` (format YYYY-MM-DD)
- **Retry** : 1 tentative avec 5 minutes de d√©lai
- **Catchup** : D√©sactiv√© (ne rattrape pas les ex√©cutions pass√©es)

### Structure du DAG

```
fetch_offers_to_gcs
        ‚Üì
transform_to_bigquery_silver
        ‚Üì
transform_to_bigquery_gold
```

## üîß Troubleshooting

### Probl√®me de permissions

```bash
# Donner les permissions aux dossiers
chmod -R 777 logs/ plugins/

# Ou changer l'ownership
chown -R $(id -u):$(id -g) logs/ plugins/
```

### Image offre-ingestion non trouv√©e

```bash
# Rebuilder l'image depuis la racine du projet
cd /home/mohamede.madiouni/jobmatch
docker build -f app/offre-ingestion/Dockerfile -t offre-ingestion-pipeline:latest .
```

### Erreur de connexion Docker

Le conteneur Airflow doit avoir acc√®s au socket Docker :
- V√©rifier que `/var/run/docker.sock` est bien mont√© dans docker-compose.yml
- V√©rifier les permissions : `ls -la /var/run/docker.sock`

### Credentials GCP non trouv√©s

```bash
# V√©rifier le chemin dans .env
cat .env.local | grep GOOGLE_APPLICATION_CREDENTIALS

# V√©rifier que le fichier existe
ls -la /chemin/vers/gcp-key.json

# Mettre √† jour la variable Airflow
docker compose exec airflow-webserver airflow variables set \
  GOOGLE_APPLICATION_CREDENTIALS \
  /opt/airflow/credentials/gcp-key.json
```

### Configuration Auto-Shutdown de la VM

Le DAG peut automatiquement √©teindre la VM GCP apr√®s succ√®s du pipeline pour r√©duire les co√ªts.

**Pr√©requis :**
1. Configurer les variables dans `.env` :
   ```bash
   GCP_VM_NAME=votre-vm-name
   GCP_VM_ZONE=europe-west9-b
   ```

2. Donner les permissions au service account :
   ```bash
   gcloud projects add-iam-policy-binding jobmatch-482415 \
     --member="serviceAccount:VOTRE_SERVICE_ACCOUNT@jobmatch-482415.iam.gserviceaccount.com" \
     --role="roles/compute.instanceAdmin.v1"
   ```

3. Rebuilder l'image Airflow pour inclure gcloud CLI :
   ```bash
   docker compose build
   docker compose up -d
   ```

**Comment √ßa fonctionne :**
- Le callback `shutdown_vm_on_success` s'ex√©cute uniquement si tout le DAG r√©ussit
- Un d√©lai de 30 secondes permet √† Airflow de finaliser les logs
- La VM s'√©teint via l'API GCP de mani√®re propre et auditable
- En cas d'√©chec du pipeline, la VM reste allum√©e pour debug

## üìö Documentation

- [Apache Airflow](https://airflow.apache.org/docs/)
- [DockerOperator](https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/operators/docker.html)
- [Pipeline offre-ingestion](../offre-ingestion/docs/README.md)

## üîê S√©curit√©

- Ne commitez **jamais** le fichier `.env.local` avec vos credentials
- Les credentials GCP sont mont√©s en **read-only** dans les conteneurs
- Changez les mots de passe par d√©faut dans `.env.local` pour la production
