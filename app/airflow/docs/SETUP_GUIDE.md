# üöÄ Guide de Setup Airflow - JobMatch

Ce guide vous permet de configurer Airflow sur votre machine en partant de z√©ro.

## üìã Pr√©requis

- **Docker** et **Docker Compose** install√©s
- **Git** pour cloner le projet
- **Credentials GCP** (fichier JSON de service account)
- **API France Travail** (Client ID et Secret)
- Minimum **4 GB de RAM** disponibles pour Docker

## üèÅ Setup complet (nouvel utilisateur)

### 1Ô∏è‚É£ Cloner le projet

```bash
git clone <URL_DU_REPO>
cd jobmatch/app/airflow
```

### 2Ô∏è‚É£ Configurer les variables d'environnement

```bash
# Copier le template
cp .env.example .env

# √âditer avec vos valeurs
nano .env  # ou vim, code, etc.
```

**Variables OBLIGATOIRES √† modifier :**
- `HOST_AIRFLOW_PATH` : Chemin absolu vers votre dossier airflow
- `AIRFLOW_UID` : Votre UID Linux (obtenu avec `echo $(id -u)`)
- `GOOGLE_APPLICATION_CREDENTIALS` : Chemin vers votre cl√© GCP
- `SMTP_USER` et `SMTP_PASSWORD` : Pour les notifications email
- `FT_CLIENT_ID` et `FT_CLIENT_SECRET` : Credentials API France Travail

**Exemple de configuration :**
```bash
HOST_AIRFLOW_PATH=/home/jean.dupont/jobmatch/app/airflow
AIRFLOW_UID=1000
SMTP_USER=jean.dupont@gmail.com
SMTP_PASSWORD=abcd efgh ijkl mnop
```

### 3Ô∏è‚É£ Ajouter les credentials GCP

```bash
# Cr√©er le dossier credentials
mkdir -p ./credentials

# Copier votre fichier de service account GCP
cp /chemin/vers/votre/cle-gcp.json ./credentials/gcp-service-account-key.json

# V√©rifier les permissions
chmod 600 ./credentials/gcp-service-account-key.json
```

### 4Ô∏è‚É£ Configurer le fichier .env dans offre-ingestion

```bash
cd ../offre-ingestion

# Copier le template
cp .env.example .env

# √âditer avec les m√™mes valeurs que pour Airflow
nano .env
```

**Important :** Les credentials doivent aussi √™tre copi√©s dans offre-ingestion :
```bash
mkdir -p ./credentials
cp /chemin/vers/votre/cle-gcp.json ./credentials/gcp-service-account-key.json
```

### 5Ô∏è‚É£ Construire l'image Docker offre-ingestion

```bash
# Retour √† la racine du projet
cd /home/VOTRE_USERNAME/jobmatch

# Build de l'image (depuis la racine car d√©pendance de 'shared')
docker build -f app/offre-ingestion/Dockerfile -t offre-ingestion-pipeline:latest .
```

Cette commande :
- Installe le package `jobmatch-shared` avec les embeddings
- Installe les d√©pendances Python
- Configure le entrypoint pour router les commandes (fetch/silver/gold)

**V√©rification :**
```bash
docker images | grep offre-ingestion
# Doit afficher : offre-ingestion-pipeline:latest
```

### 6Ô∏è‚É£ Configurer les permissions (Linux uniquement)

```bash
cd app/airflow

# Cr√©er les dossiers n√©cessaires
mkdir -p ./logs ./plugins ./config

# Donner les permissions
chmod -R 777 ./logs ./plugins
```

### 7Ô∏è‚É£ Lancer Airflow

```bash
# Build et d√©marrage
docker compose up -d

# V√©rifier que tous les services sont UP
docker compose ps
```

Vous devriez voir 4 conteneurs :
- `airflow-webserver` (port 8080)
- `airflow-scheduler`
- `airflow-init` (√©tat: Exited 0)
- `postgres`

### 8Ô∏è‚É£ Acc√©der √† l'interface Airflow

```
URL: http://localhost:8080
Username: airflow
Password: airflow
```

(√Ä moins que vous ayez chang√© dans .env)

### 9Ô∏è‚É£ Activer et tester le DAG

1. Dans l'interface, chercher le DAG `offre_ingestion_pipeline`
2. Activer le toggle (bouton ON/OFF)
3. Cliquer sur le bouton ‚ñ∂Ô∏è (Play) pour lancer manuellement
4. Observer l'ex√©cution dans la vue Graph ou Grid

## üß™ Test de validation complet

Ex√©cutez ces commandes pour valider que tout fonctionne :

```bash
# 1. V√©rifier que les conteneurs sont actifs
docker compose ps

# 2. V√©rifier les logs du scheduler
docker compose logs airflow-scheduler --tail=50

# 3. Tester l'acc√®s au webserver
curl http://localhost:8080/health

# 4. V√©rifier que l'image offre-ingestion est disponible
docker images | grep offre-ingestion

# 5. Tester l'ex√©cution d'un script du pipeline (en dehors d'Airflow)
docker run --rm \
  -v $PWD/../offre-ingestion/credentials/gcp-service-account-key.json:/app/credentials/gcp-key.json:ro \
  -v $PWD/../offre-ingestion/.env:/app/src/.env:ro \
  -e GOOGLE_APPLICATION_CREDENTIALS=/app/credentials/gcp-key.json \
  offre-ingestion-pipeline:latest fetch --help
```

## üêõ Troubleshooting

### Erreur : "Permission denied" sur /var/run/docker.sock

```bash
# Ajouter votre utilisateur au groupe docker
sudo usermod -aG docker $USER

# Red√©marrer la session
newgrp docker
```

### Erreur : "Image offre-ingestion-pipeline not found"

```bash
# Rebuilder l'image depuis la racine
cd /chemin/vers/jobmatch
docker build -f app/offre-ingestion/Dockerfile -t offre-ingestion-pipeline:latest .
```

### Erreur : "Credentials not found"

V√©rifier que le chemin dans `.env` pointe vers le bon fichier :
```bash
ls -la $(grep GOOGLE_APPLICATION_CREDENTIALS .env | cut -d'=' -f2 | tr -d '"')
```

### Le DAG ne d√©marre pas

```bash
# Consulter les logs du scheduler
docker compose logs airflow-scheduler -f

# V√©rifier que le fichier DAG n'a pas d'erreur
docker compose exec airflow-scheduler python -c "from dags.offre_ingestion_dag import dag"
```

### Les t√¢ches Docker √©chouent

```bash
# V√©rifier l'acc√®s au socket Docker depuis le conteneur
docker compose exec airflow-scheduler docker ps

# Si erreur, v√©rifier les permissions
ls -la /var/run/docker.sock
```

## üìä Surveillance et logs

```bash
# Logs en temps r√©el
docker compose logs -f

# Logs d'un service sp√©cifique
docker compose logs airflow-scheduler -f

# Logs d'une t√¢che Airflow (dans l'interface web)
http://localhost:8080/dags/offre_ingestion_pipeline/grid
```

## üõë Arr√™ter Airflow

```bash
# Arr√™t propre
docker compose down

# Arr√™t avec suppression des volumes (‚ö†Ô∏è perte des donn√©es)
docker compose down -v
```

## üîÑ Mise √† jour

Quand le code change :

```bash
# 1. Arr√™ter Airflow
docker compose down

# 2. Rebuild l'image offre-ingestion si n√©cessaire
cd /chemin/vers/jobmatch
docker build -f app/offre-ingestion/Dockerfile -t offre-ingestion-pipeline:latest .

# 3. Rebuild Airflow si le Dockerfile a chang√©
cd app/airflow
docker compose build

# 4. Red√©marrer
docker compose up -d
```

## üìö Ressources

- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Pipeline offre-ingestion](../offre-ingestion/docs/README.md)
- [DockerOperator Guide](https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/operators/docker.html)
