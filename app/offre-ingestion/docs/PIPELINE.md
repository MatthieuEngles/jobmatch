# Pipeline d'Ingestion des Offres France Travail

## ğŸ¯ Vue d'ensemble

Pipeline automatisÃ© d'ingestion des offres d'emploi depuis l'API France Travail vers Google Cloud Platform (GCS + BigQuery) avec gÃ©nÃ©ration d'embeddings vectoriels.

## ğŸ—ï¸ Architecture MÃ©daillon

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  France Travail â”‚â”€â”€â”€â”€â–¶â”‚    Bronze    â”‚â”€â”€â”€â”€â–¶â”‚    Silver    â”‚â”€â”€â”€â”€â–¶â”‚     Gold     â”‚
â”‚      API        â”‚     â”‚     GCS      â”‚     â”‚   BigQuery   â”‚     â”‚  BigQuery +  â”‚
â”‚                 â”‚     â”‚  (JSON brut) â”‚     â”‚  (StructurÃ©) â”‚     â”‚  Embeddings  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ Bronze Layer : Extraction vers GCS

### Script
`src/pipelines/fetch_offers_to_gcs.py`

### Objectif
Extraire les offres d'emploi depuis l'API France Travail et les stocker dans Google Cloud Storage.

### Processus
1. **Authentification OAuth2** avec cache token (30 minutes)
2. **Parcours des codes ROME** : 1585 mÃ©tiers rÃ©fÃ©rencÃ©s
3. **Pagination automatique** : 150 offres par requÃªte
4. **Throttling** : respect intervalle 0.11s entre requÃªtes
5. **Gestion erreurs** : retry automatique sur 401/429
6. **Upload GCS** : stockage JSON brut partitionnÃ© par date

### ExÃ©cution

```bash
# Docker (recommandÃ©)
docker compose run --rm offre-ingestion fetch 2025-12-31

# Par dÃ©faut : donnÃ©es de J-1
docker compose run --rm offre-ingestion fetch
```

### Configuration

Fichier `.env` :
```env
# GCP
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
GCP_PROJECT_ID=your-project-id
GCS_BUCKET_NAME=your-bucket-name

# France Travail API
FT_CLIENT_ID=your_client_id
FT_CLIENT_SECRET=your_client_secret
FT_SCOPE=the_list_of_scopes
FT_OAUTH_URL=the_oauth_url
FT_API_URL_BASE=the_api_base_url
FT_ROMECODES_PATH=/path/to/romecodes.txt
```

### Sortie

**GCS** : `gs://france-travail-bronze-offers/france_travail/offers/ingestion_date=YYYY-MM-DD/offers_YYYY-MM-DD.json`

Structure JSON :
```json
{
  "resultats": [
    {
      "id": "201VPGR",
      "intitule": "DÃ©veloppeur Python H/F",
      "description": "...",
      "dateCreation": "2025-12-31T10:30:00Z",
      "lieuTravail": {...},
      "entreprise": {...},
      "competences": [...]
    }
  ]
}
```

---

## 2ï¸âƒ£ Silver Layer : Transformation BigQuery

### Script
`src/pipelines/transform_offers_to_bigquery_silver.py`

### Objectif
Transformer les fichiers JSON bruts depuis GCS en tables BigQuery structurÃ©es et normalisÃ©es.

### Processus
1. **Lecture depuis GCS** : chargement du JSON brut de la date cible
2. **Normalisation** : structuration en 13 tables relationnelles
3. **Insertion BigQuery** : chargement batch avec gestion des doublons
4. **Indexation** : crÃ©ation d'index sur `offer_id` pour performances

### ExÃ©cution

```bash
# Docker (recommandÃ©)
docker compose run --rm offre-ingestion silver 2025-12-31

# Par dÃ©faut : donnÃ©es de J-1
docker compose run --rm offre-ingestion silver
```

### Sortie

**BigQuery Dataset** : `silver_dataset`

**Tables crÃ©Ã©es** (13 tables) :
- `offers` (table principale, 27 colonnes)
- `offers_lieu_travail`
- `offers_entreprise`
- `offers_salaire`
- `offers_salaire_complements`
- `offers_competences`
- `offers_qualites_professionnelles`
- `offers_formations`
- `offers_permis`
- `offers_langues`
- `offers_contact`
- `offers_origine`
- `offers_contexte_travail_horaires`

---

## 3ï¸âƒ£ Gold Layer : Embeddings Vectoriels

### Script
`src/pipelines/transform_offers_to_bigquery_gold.py`

### Objectif
GÃ©nÃ©rer des embeddings vectoriels (sentence-transformers) pour les champs `intitule` et `description` afin de permettre la recherche sÃ©mantique.

### Processus
1. **Lecture BigQuery Silver** : extraction des offres de la date cible
2. **GÃ©nÃ©ration embeddings** :
   - ModÃ¨le : `sentence-transformers/all-MiniLM-L6-v2`
   - Dimension : 384
   - Batch processing pour optimisation
3. **Stockage BigQuery Gold** :
   - Table `offers` (donnÃ©es mÃ©tier)
   - Table `offers_intitule_embeddings` (vecteurs titres)
   - Table `offers_description_embeddings` (vecteurs descriptions)

### ExÃ©cution

```bash
# Docker (recommandÃ©)
docker compose run --rm offre-ingestion gold 2025-12-31

# Par dÃ©faut : donnÃ©es de J-1
docker compose run --rm offre-ingestion gold
```

### Sortie

**BigQuery Dataset** : `gold_dataset`

**Tables crÃ©Ã©es** :
- `offers` : DonnÃ©es mÃ©tier
- `offers_intitule_embeddings` : Vecteurs 384D des titres
- `offers_description_embeddings` : Vecteurs 384D des descriptions

**Index vectoriels** : CrÃ©Ã©s automatiquement pour recherche sÃ©mantique rapide

---

## ğŸ“‹ Structure des DonnÃ©es

### Table principale : `offers` (Silver & Gold)

| Colonne | Type | Description |
|---------|------|-------------|
| `id` | STRING | Identifiant unique (clÃ© primaire) |
| `intitule` | STRING | Titre du poste |
| `description` | STRING | Description complÃ¨te |
| `romeCode` | STRING | Code ROME du mÃ©tier |
| `romeLibelle` | STRING | LibellÃ© du mÃ©tier |
| `typeContrat` | STRING | CDI, CDD, MIS, etc. |
| `experienceExige` | STRING | D=DÃ©butant, E=ExpÃ©rimentÃ©, S=SouhaitÃ© |
| `dateCreation` | TIMESTAMP | Date de crÃ©ation |
| `dateActualisation` | TIMESTAMP | DerniÃ¨re mise Ã  jour |
| `nombrePostes` | INTEGER | Nombre de postes |
| `accessibleTH` | BOOLEAN | Accessible handicap |
| ... | ... | (27 colonnes au total) |

### Tables secondaires (reliÃ©es par `offer_id`)

Toutes les tables ont une clÃ© Ã©trangÃ¨re `offer_id` pointant vers `offers.id`.

---

## âš¡ Performances

- **Bronze** : ~30-60 secondes pour 1584 codes ROME (selon volume)
- **Silver** : ~10-20 secondes pour transformation et insertion
- **Gold** : ~20-60 min pour gÃ©nÃ©ration embeddings (dÃ©pend du nombre d'offres)

---

## ğŸ› ï¸ Maintenance

### Scripts de setup

```bash
# CrÃ©er les schÃ©mas BigQuery
python scripts/setup/create_bigquery_silver_schema.py
python scripts/setup/create_bigquery_gold_schema.py

# CrÃ©er les index vectoriels
python scripts/setup/create_bigquery_gold_vector_indexes.py
```

### Scripts utilitaires

```bash
# Compter les offres dans GCS
python scripts/utils/count_total_offers_in_gcs.py

# Lire les offres depuis GCS
python scripts/utils/read_offers_from_gcs.py 2025-12-31
```

---

## Notes Importantes

### Filtres de date

L'API France Travail retourne les offres crÃ©Ã©es entre `minCreationDate` et `maxCreationDate`.
Par dÃ©faut, le script rÃ©cupÃ¨re les offres crÃ©Ã©es le jour J-1 (24h).

---

## Support

Pour toute question sur le pipeline, consulter :
- `README.md` dans ce rÃ©pertoire
- `GUIDE_REQUETES.md` pour l'utilisation de la base de donnÃ©es
- Les scripts eux-mÃªmes (bien documentÃ©s)
